{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Infrastructure","text":"<p>Flux based GitOps repository for my home lab infrastructure.</p> <p> </p> <p> </p>"},{"location":"#requirements","title":"Requirements","text":"<ul> <li>Task to execute all required commands</li> <li>Node.js + Yarn (for local repository management only)</li> <li>Ansible to provision the cluster nodes with common settings, Tailscale and K3s</li> <li>Terraform to deploy all external DNS records</li> <li>Flux which manages and updates the cluster state</li> </ul>"},{"location":"#installation","title":"Installation","text":"<p>The cluster can be set up using the following commands:</p> <pre><code># clone the repo to your local machine\n$ git clone https://github.com/pascaliske/infrastructure\n\n# install needed dependencies\n$ task install\n\n# setup ansible vault file (one-time)\n$ task vault:setup\n\n# provision nodes using ansible (1)\n$ task cluster:provision\n\n# bootstrap flux cluster\n$ task cluster:bootstrap -- \\\n    --owner=$GITHUB_USER \\ # required flag (2)\n    --repository=$GITHUB_REPO \\ # required flag (3)\n    --branch=main\n</code></pre> <ol> <li>More information on the following command can be found in the provisioning section.</li> <li>Ensure you either fill in your GitHub username or you make it available as an environment variable.</li> <li>Name of the repository to hold the declarative cluster state. If it does not exists yet, it will automatically be created by Flux.</li> </ol>"},{"location":"#updates","title":"Updates","text":"<p>Most updates inside this project are automated. Take a look at the update section for detailed information on this topic.</p>"},{"location":"#thanks","title":"Thanks","text":"<p>A big thank you goes to these awesome people and their projects who inspired me to do this project:</p> <ul> <li>onedr0p/home-ops</li> <li>nicholaswilde/home-cluster</li> <li>billimek/k8s-gitops</li> </ul> <p>Also I want to thank you the awesome <code>k8s-at-home</code> community for all their work on their Helm Charts which helped me a lot.</p>"},{"location":"#license","title":"License","text":"<p>MIT \u2013 \u00a9 2024 Pascal Iske</p>"},{"location":"LICENSE/","title":"The MIT License","text":"<p>Copyright 2024 Pascal Iske, https://pascaliske.dev</p> <p>Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the \"Software\"), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions:</p> <p>The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software.</p> <p>THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.</p>"},{"location":"cluster/base/","title":"Overview","text":"<p>The cluster represents the heart of this project. Since I wanted to keep things lightweight and relatively simple, I chose K3s as the underlying Kubernetes distribution. The installation of the multi-node cluster is straightforward and is performed by Ansible during the provisioning state.</p> <p>The cluster state is managed by Flux in a GitOps based manner. To provide a clean separation of concerns the cluster is divided into several sections reflected as folders in the repository.</p> <p></p> <p>The <code>base</code> directory serves as an entrypoint from which all other sections are executed. It manages all Flux related components and afterwards reconciles the following <code>Kustomization</code> files in exactly this order:</p> <ul> <li>Config \u2014 Cluster-wide configuration values</li> <li>CRDs \u2014 Custom Resource Definitions for specific applications</li> <li>Charts \u2014 Helm chart repositories for all applications</li> <li>Core \u2014 Crucial core applications which depend on Config, Charts &amp; CRDs</li> <li>Services \u2014 Regular applications which depend on Config, Charts, CRDs &amp; Core</li> </ul>"},{"location":"cluster/charts/","title":"Charts","text":"<p>All services inside the cluster are deployed by Flux using Helm charts. To ensure the existence of all chart repositories before the service reconciliation they're grouped together and applied as a <code>Kustomization</code> dependency for all services.</p> <p>The following chart repositories will be created as <code>HelmRepository</code> declarations by this section:</p> <ul> <li><code>authelia</code> \u2192 https://charts.authelia.com</li> <li><code>cloudflare-exporter</code> \u2192 https://lablabs.github.io/cloudflare-exporter</li> <li><code>giantswarm</code> \u2192 https://giantswarm.github.io/control-plane-catalog/</li> <li><code>gitlab</code> \u2192 https://charts.gitlab.io</li> <li><code>grafana</code> \u2192 https://grafana.github.io/helm-charts</li> <li><code>jetstack</code> \u2192 https://charts.jetstack.io</li> <li><code>k8s-at-home</code> \u2192 https://k8s-at-home.com/charts</li> <li><code>kured</code> \u2192 https://kubereboot.github.io/charts</li> <li><code>pascaliske</code> \u2192 https://charts.pascaliske.dev</li> <li><code>prometheus-community</code> \u2192 https://prometheus-community.github.io/helm-charts</li> <li><code>traefik</code> \u2192 https://helm.traefik.io/traefik</li> <li><code>vector</code> \u2192 https://helm.vector.dev</li> <li><code>vmware-tanzu</code> \u2192 https://vmware-tanzu.github.io/helm-charts</li> </ul> <p>Flux checks these helm repositories regularly at the specified intervals of <code>10m0s</code> and updates the <code>HelmRelease</code> objects accordingly.</p> Example of a <code>HelmRepository</code> declaration <pre><code>apiVersion: source.toolkit.fluxcd.io/v1\nkind: HelmRepository\nmetadata:\n  name: pascaliske\n  namespace: flux-system\nspec:\n  interval: 10m0s\n  url: https://charts.pascaliske.dev\n</code></pre>"},{"location":"cluster/config/","title":"Config","text":"<p>This section provides global configuration values for all services, e.g. the cluster internal domain.</p> <p>It creates a <code>ConfigMap</code> object called <code>cluster-settings</code> with regular values and a <code>Secret</code> object called <code>cluster-secrets</code> containing secret values. The latter is encrypted in-place using Mozillas SOPS tool.</p> <p>See this guide for more information on the usage of SOPS in combination with Flux.</p>"},{"location":"cluster/core/","title":"Core","text":"<p>This section contains crucial core services which are required by all other services, for example the ingress controller. The services are grouped inside folders by their namespaces and will be deployed before any other service is reconciliated.</p> <p>The following services are deployed by this section:</p> <ul> <li><code>system-upgrade</code></li> <li><code>flux-system</code></li> <li><code>kured</code></li> <li><code>traefik</code></li> <li><code>traefik-errors</code></li> <li><code>cert-manager</code></li> <li><code>cloudnative-pg</code></li> <li><code>crowdsec</code></li> <li><code>velero</code></li> <li><code>trivy-operator</code></li> <li><code>silence-operator</code></li> </ul>"},{"location":"cluster/core/cert-manager/","title":"<code>cert-manager</code>","text":""},{"location":"cluster/core/cert-manager/#introduction","title":"Introduction","text":"<p><code>cert-manager</code> is used to manage certificates inside the cluster. It provides CRDs for automated requests of Let's Encrypt certificates for domains. Right before the certificates reach their expiration date, cert-manager also takes care of renewing them for me.</p> Example of kind <code>Certificate</code> <pre><code>apiVersion: cert-manager.io/v1\nkind: Certificate\nmetadata:\n  name: auth.${DOMAIN_INTERNAL}\n  namespace: authelia\nspec:\n  secretName: auth.${DOMAIN_INTERNAL}\n  dnsNames:\n    - auth.${DOMAIN_INTERNAL}\n  issuerRef:\n    kind: ClusterIssuer\n    name: lets-encrypt-production\n</code></pre> <p>Due to the fact that the target domains are not reachable outside my home network cert-manager is configured to use the DNS challenge for verifying the ownership.</p>"},{"location":"cluster/core/cert-manager/#created-resources","title":"Created Resources","text":"Kind Name <code>Namespace</code> <code>cert-manager</code> <code>HelmRelease</code> <code>cert-manager</code> <code>Secret</code> <code>cloudflare-api-token</code> <code>ClusterIssuer</code> <code>lets-encrypt-production</code>, <code>lets-encrypt-staging</code>"},{"location":"cluster/core/cert-manager/#cli","title":"CLI","text":"<p><code>cert-manager</code> has a great CLI tool to interact with the controller running inside the cluster. The installation guide can be found here.</p> Example usage of <code>cert-manager</code> CLI Manually renew certificate(s)Get the status of a certificate <p><pre><code>$ cmctl renew &lt;certificate&gt;\n</code></pre> You can find more information on this command in their docs.</p> <p><pre><code>$ cmctl status certificate -n &lt;namespace&gt; &lt;certificate&gt;\n</code></pre> You can find more information on this command in their docs.</p>"},{"location":"cluster/core/cert-manager/#links","title":"Links","text":"<ul> <li>Documentation</li> <li>GitHub Repository</li> </ul>"},{"location":"cluster/core/cloudnative-pg/","title":"<code>cloudnative-pg</code>","text":""},{"location":"cluster/core/cloudnative-pg/#introduction","title":"Introduction","text":"<p>The <code>cloudnative-pg</code> controller is leveraged to deploy multiple PostgreSQL databases which are used for various services within the cluster.</p> <p>It\u2019s entirely declarative, and directly integrates with the Kubernetes API server to update the state of the cluster \u2014 for this reason, it does not require an external failover management tool. \u2014 https://cloudnative-pg.io</p> Example of kind <code>Cluster</code> <pre><code>apiVersion: postgresql.cnpg.io/v1\nkind: Cluster\nmetadata:\n  name: postgresql\n  namespace: my-app\nspec:\n  instances: 1\n  primaryUpdateStrategy: unsupervised\n  storage:\n    size: 1Gi\n  superuserSecret:\n      name: postgresql-superuser\n    bootstrap:\n      initdb:\n        database: my-app\n        owner: my-app\n        secret:\n          name: postgresql-user\n    monitoring:\n      enablePodMonitor: true\n</code></pre>"},{"location":"cluster/core/cloudnative-pg/#created-resources","title":"Created Resources","text":"Kind Name <code>Namespace</code> <code>cloudnative-pg</code> <code>HelmRelease</code> <code>cloudnative-pg</code>"},{"location":"cluster/core/cloudnative-pg/#links","title":"Links","text":"<ul> <li>Documentation</li> <li>Helm Chart</li> <li>GitHub Repository</li> </ul>"},{"location":"cluster/core/crowdsec/","title":"<code>crowdsec</code>","text":""},{"location":"cluster/core/crowdsec/#introduction","title":"Introduction","text":""},{"location":"cluster/core/crowdsec/#created-resources","title":"Created Resources","text":"Kind Name <code>Namespace</code> <code>crowdsec</code> <code>HelmRelease</code> <code>crowdsec</code> <code>Secret</code> <code>crowdsec-keys</code>"},{"location":"cluster/core/crowdsec/#links","title":"Links","text":"<ul> <li>Documentation</li> <li>Helm Chart</li> <li>GitHub Repository</li> </ul>"},{"location":"cluster/core/flux-system/","title":"<code>flux-system</code>","text":""},{"location":"cluster/core/flux-system/#introduction","title":"Introduction","text":"<p>Flux is configured to notify me about important events like reconciliation errors via Discord. It also sends the current reconciliation status of all sections to GitHub as status checks, which are nicely displayed inside PRs:</p> <p></p> <p>For a complete guide of how to setup this notifications visit their documentation.</p>"},{"location":"cluster/core/flux-system/#created-resources","title":"Created Resources","text":"Kind Name <code>Provider</code> <code>github</code>, <code>discord</code>, <code>grafana</code> <code>Alert</code> <code>github</code>, <code>infrastructure</code>, <code>grafana</code> <code>Secret</code> <code>github-token</code>, <code>discord-webhook-url</code>, <code>grafana-auth</code>"},{"location":"cluster/core/flux-system/#links","title":"Links","text":"<ul> <li>Documentation</li> <li>Guide</li> </ul>"},{"location":"cluster/core/kured/","title":"<code>kured</code>","text":""},{"location":"cluster/core/kured/#introduction","title":"Introduction","text":"<p>Kured is a Kubernetes <code>DaemonSet</code> that performs safe automatic node reboots when the need to do so is indicated by the package management system of the underlying OS.</p> <ul> <li>Watches for the presence of a reboot sentinel file e.g. <code>/var/run/reboot-required</code> or the successful run of a sentinel command.</li> <li>Utilizes a lock in the API server to ensure only one node reboots at a time</li> <li>Optionally defers reboots in the presence of active Prometheus alerts or selected pods</li> <li>Cordons &amp; drains worker nodes before reboot, uncordoning them after</li> </ul>"},{"location":"cluster/core/kured/#created-resources","title":"Created Resources","text":"Kind Name <code>Namespace</code> <code>kured</code> <code>HelmRelease</code> <code>kured</code>"},{"location":"cluster/core/kured/#links","title":"Links","text":"<ul> <li>GitHub Repository</li> </ul>"},{"location":"cluster/core/node-problem-detector/","title":"<code>node-problem-detector</code>","text":""},{"location":"cluster/core/node-problem-detector/#introduction","title":"Introduction","text":""},{"location":"cluster/core/node-problem-detector/#created-resources","title":"Created Resources","text":"Kind Name <code>HelmRelease</code> <code>node-problem-detector</code>"},{"location":"cluster/core/node-problem-detector/#links","title":"Links","text":"<ul> <li>Documentation</li> <li>Helm Chart</li> <li>GitHub Repository</li> </ul>"},{"location":"cluster/core/redis-operator/","title":"<code>redis-operator</code>","text":""},{"location":"cluster/core/redis-operator/#introduction","title":"Introduction","text":"<p>tbd</p>"},{"location":"cluster/core/redis-operator/#created-resources","title":"Created Resources","text":"Kind Name <code>Namespace</code> <code>redis-operator</code> <code>HelmRelease</code> <code>redis-operator</code>"},{"location":"cluster/core/redis-operator/#links","title":"Links","text":"<ul> <li>Documentation</li> <li>GitHub Repository</li> <li>Helm Chart</li> </ul>"},{"location":"cluster/core/silence-operator/","title":"<code>silence-operator</code>","text":""},{"location":"cluster/core/silence-operator/#introduction","title":"Introduction","text":"<p>The Silence Operator automates the management of Alertmanager silences using Kubernetes Custom Resources.</p>"},{"location":"cluster/core/silence-operator/#created-resources","title":"Created Resources","text":"Kind Name <code>Namespace</code> <code>silence-operator</code> <code>HelmRelease</code> <code>silence-operator</code>"},{"location":"cluster/core/silence-operator/#links","title":"Links","text":"<ul> <li>Documentation</li> <li>GitHub Repository</li> <li>Helm Chart</li> </ul>"},{"location":"cluster/core/system-upgrade/","title":"<code>system-upgrade</code>","text":""},{"location":"cluster/core/system-upgrade/#introduction","title":"Introduction","text":"<p>Rancher's <code>system-upgrade-controller</code> makes it easy to upgrade K3s nodes in an automated way. It introduces a <code>CustomResourceDefinition</code> named <code>Plan</code> to provide details about an update of a selected range of nodes:</p> <p>Example of kind <code>Plan</code></p> Plan/system-upgrade/master<pre><code>---\napiVersion: upgrade.cattle.io/v1\nkind: Plan\nmetadata:\n  name: master\n  namespace: system-upgrade\nspec:\n  # renovate: datasource=github-releases depName=k3s-io/k3s (1)\n  version: \"v1.33.3+k3s1\"\n  serviceAccountName: system-upgrade\n  concurrency: 1\n  cordon: true\n  nodeSelector:\n    matchExpressions:\n      # only control-plane nodes (2)\n      - key: node-role.kubernetes.io/control-plane\n        operator: Exists\n  tolerations:\n    - key: node-role.kubernetes.io/control-plane\n      operator: Exists\n      effect: NoSchedule\n  upgrade:\n    image: rancher/k3s-upgrade\n</code></pre> <ol> <li>You can use Renovate Bot to automate upgrades of K3s using plans</li> <li>Labels and selectors are used to differentiate between control-plane nodes and worker ones</li> </ol> <p>The controller then picks up any <code>Plan</code> resource and performs the defined upgrades accordingly inside the cluster. For more information you can check out the official K3s docs or the GitHub Repository.</p>"},{"location":"cluster/core/system-upgrade/#created-resources","title":"Created Resources","text":"Kind Name <code>Namespace</code> <code>system-upgrade</code> <code>Plan</code> <code>master</code>, <code>worker</code>"},{"location":"cluster/core/system-upgrade/#links","title":"Links","text":"<ul> <li>GitHub Repository</li> <li>K3s docs</li> </ul>"},{"location":"cluster/core/traefik/","title":"<code>traefik</code>","text":""},{"location":"cluster/core/traefik/#introduction","title":"Introduction","text":"<p>Traefik is used as ingress controller for the cluster. To allow more flexible configurations I disabled the built-in Traefik installation of K3s using their configuration file (which is provisioned using Ansible) and installed it separately with a custom <code>HelmRelease</code> found in this <code>Namespace</code>.</p> /etc/rancher/k3s/config.yaml<pre><code>disable:\n  - traefik\n</code></pre> <p>Traefik provides a custom resource definition <code>IngressRoute</code> for routing ingress traffic from the outside world to <code>Service</code> objects inside the cluster. Additionally Traefik provides a custom resource definition to implement <code>Middleware</code> objects which allow you to modify the requests and responses of your ingress objects.</p> Example of kind <code>IngressRoute</code> <pre><code>apiVersion: traefik.io/v1alpha1\nkind: IngressRoute\nmetadata:\n  name: dashboard\n  namespace: authelia\nspec:\n  entryPoints:\n    - https\n  routes:\n    - kind: Rule\n      match: Host(`auth.${DOMAIN_INTERNAL}`)\n      services:\n        - kind: Service\n          name: authelia\n          namespace: authelia\n          port: 80\n      middlewares:\n        - name: security\n          namespace: traefik\n  tls:\n    secretName: auth.${DOMAIN_INTERNAL}\n</code></pre> Example of kind <code>Middleware</code> <pre><code>apiVersion: traefik.io/v1alpha1\nkind: Middleware\nmetadata:\n  name: headers\n  namespace: authelia\nspec:\n  headers:\n    browserXssFilter: true\n    customFrameOptionsValue: SAMEORIGIN\n    customResponseHeaders:\n      Cache-Control: no-store\n      Pragma: no-cache\n</code></pre> <p>The dashboard of Traefik is enabled and needs to be accessible by domain. This <code>Kustomization</code> creates the required resources for that. It also creates a <code>ServiceMonitor</code> for the prometheus operator to pick up and monitor the traefik instance.</p>"},{"location":"cluster/core/traefik/#created-resources","title":"Created Resources","text":"Kind Name <code>Namespace</code> <code>traefik</code> <code>HelmRelease</code> <code>traefik</code> <code>Service</code> <code>traefik-metrics</code> <code>Certificate</code> <code>traefik.${DOMAIN_INTERNAL}</code> <code>IngressRoute</code> <code>dashboard</code> <code>ServiceMonitor</code> <code>traefik</code>"},{"location":"cluster/core/traefik/#links","title":"Links","text":"<ul> <li>Documentation</li> <li>GitHub Repository</li> <li>Helm Chart</li> </ul>"},{"location":"cluster/core/traefik-errors/","title":"<code>traefik-errors</code>","text":""},{"location":"cluster/core/traefik-errors/#introduction","title":"Introduction","text":"<p>If for some reason a request to any service fails a custom error page is shown. This is possible due to a <code>Middleware</code> and the <code>traefik-errors</code> service. For more information see here.</p>"},{"location":"cluster/core/traefik-errors/#created-resources","title":"Created Resources","text":"Kind Name <code>Namespace</code> <code>traefik-errors</code> <code>HelmRelease</code> <code>traefik-errors</code>"},{"location":"cluster/core/traefik-errors/#links","title":"Links","text":"<ul> <li>Documentation</li> <li>GitHub Repository</li> <li>Helm Chart</li> </ul>"},{"location":"cluster/core/trivy-operator/","title":"<code>trivy-operator</code>","text":""},{"location":"cluster/core/trivy-operator/#introduction","title":"Introduction","text":"<p>The <code>trivy-operator</code> leverages trivy security tools by incorporating their outputs into Kubernetes CRDs (Custom Resource Definitions) and from there, making security reports accessible through the Kubernetes API. This way users can find and view the risks that relate to different resources in what we call a Kubernetes-native way.</p> <p>The operator automatically updates security reports in response to workload and other changes on the cluster, generating the following reports:</p> <ul> <li>Vulnerability Scans: Automated vulnerability scanning for Kubernetes workloads.</li> <li>ConfigAudit Scans: Automated configuration audits for Kubernetes resources with predefined rules or custom Open Policy Agent (OPA) policies.</li> <li>Exposed Secret Scans: Automated secret scans which find and detail the location of exposed Secrets within your cluster.</li> <li>RBAC Scans: Role Based Access Control scans provide detailed information on the access rights of the different resources installed.</li> <li>K8s core component infra assessment scan Kubernetes infra core components (etcd,apiserver,scheduler,controller-manager and etc) setting and configuration.</li> <li>Compliance reports NSA, CISA Kubernetes Hardening Guidance v1.1 cybersecurity technical report is produced.</li> </ul>"},{"location":"cluster/core/trivy-operator/#created-resources","title":"Created Resources","text":"Kind Name <code>Namespace</code> <code>trivy-operator</code> <code>HelmRelease</code> <code>trivy-operator</code>"},{"location":"cluster/core/trivy-operator/#links","title":"Links","text":"<ul> <li>Documentation</li> <li>GitHub Repository</li> <li>Helm Chart</li> </ul>"},{"location":"cluster/core/velero/","title":"<code>velero</code>","text":""},{"location":"cluster/core/velero/#introduction","title":"Introduction","text":"<p>Velero is used to easily backup all cluster internal resources. It provides CRDs for creating manual or scheduled backups.</p> <p>Currently I've scheduled Velero to create a backup automatically every Sunday at 3am. As <code>BackupStorageLocation</code> I configured a completely separate and self-hosted instance of Minio running on my Synology NAS.</p>"},{"location":"cluster/core/velero/#created-resources","title":"Created Resources","text":"Kind Name <code>Namespace</code> <code>velero</code> <code>HelmRelease</code> <code>velero</code>"},{"location":"cluster/core/velero/#cli","title":"CLI","text":"<p>Velero has a great CLI tool to interact with the controller running inside the cluster. The installation guide can be found here.</p> Example usage of Velero CLI Create a manual backupList backupsRestore a backup <pre><code>$ velero backup create &lt;name&gt;\n</code></pre> <pre><code>$ velero backup list\n</code></pre> <pre><code>$ velero restore create &lt;name&gt; --from-backup &lt;backup&gt;\n</code></pre>"},{"location":"cluster/core/velero/#links","title":"Links","text":"<ul> <li>Documentation</li> <li>GitHub Repository</li> <li>Helm Chart</li> </ul>"},{"location":"cluster/crds/","title":"CRDs","text":"<p>Some services need <code>CustomResourceDefinition</code>s to provide their functionalities. To prevent race conditions with the actual usage of those definitions, I've extracted the installation into a custom section which will be deployed before the core and services sections. Inside the corresponding <code>HelmRelease</code>s the deployment of CRDs is skipped.</p> <p>The following services require CRDs which are deployed by this section:</p> <ul> <li><code>cert-manager</code></li> <li><code>cloudnative-pg</code></li> <li><code>kube-prometheus-stack</code></li> <li><code>silence-operator</code></li> <li><code>system-upgrade</code></li> <li><code>traefik</code></li> <li><code>trivy-operator</code></li> <li><code>velero</code></li> </ul>"},{"location":"cluster/services/","title":"Services","text":"<p>This section contains all regular services  deployed in alphabetical order. The services are grouped inside folders by their namespaces and will be deployed after all core services were reconciliated and deployed.</p> <p>The following services are deployed by this section:</p> <ul> <li><code>authelia</code></li> <li><code>blocky</code></li> <li><code>cloudflared</code></li> <li><code>dyndns</code></li> <li><code>gitlab</code></li> <li><code>home-assistant</code></li> <li><code>homer</code></li> <li><code>linkding</code></li> <li><code>monitoring</code></li> <li><code>pairdrop</code></li> <li><code>paperless</code></li> <li><code>redis</code></li> <li><code>traefik</code></li> <li><code>unifi</code></li> <li><code>uptime-kuma</code></li> <li><code>vaultwarden</code></li> </ul>"},{"location":"cluster/services/authelia/","title":"<code>authelia</code>","text":""},{"location":"cluster/services/authelia/#introduction","title":"Introduction","text":"<p>Authelia is an open-source authentication and authorization server providing two-factor authentication and single sign-on (SSO) for your applications via a web portal. It acts as a companion for reverse proxies by allowing, denying, or redirecting requests.</p> <p></p> <p>Authelia allows me to centralize the authentication part of nearly all deployed services into one neat web portal. It features a complex access control rule system and enables the usage of multi-factor authentication.</p> <p>For every service which supports user authentication by HTTP headers or complete disablement of authentication, I configured Authelia as authentication layer using a Traefik middleware.</p> Example via Traefik <code>Middleware</code> <code>Middleware</code>Usage <p>First a <code>Middleware</code> object needs to be created which defines the <code>forwardAuth</code> address and headers:</p> <pre><code>---\napiVersion: traefik.io/v1alpha1\nkind: Middleware\nmetadata:\n  name: auth\n  namespace: traefik\nspec:\n  forwardAuth:\n    address: http://authelia.authelia.svc.cluster.local/api/authz/forward-auth\n    trustForwardHeader: true\n    authResponseHeaders:\n      - Remote-User\n      - Remote-Groups\n      - Remote-Name\n      - Remote-Email\n</code></pre> <p>In order for the <code>Middleware</code> to take effect, it must be added to the middlewares section of any <code>IngressRoute</code>:</p> <pre><code>---\napiVersion: traefik.io/v1alpha1\nkind: IngressRoute\nmetadata:\n  name: dashboard\n  namespace: traefik\nspec:\n  entryPoints:\n    - https\n  routes:\n    - kind: Rule\n      match: Host(`traefik.${DOMAIN_INTERNAL}`)\n      services:\n        - kind: TraefikService\n          name: api@internal\n      middlewares:\n        - name: auth\n          namespace: traefik\n        - name: security\n          namespace: traefik\n        - name: optimizations\n          namespace: traefik\n        - name: error-pages\n          namespace: traefik-errors\n  tls:\n    secretName: traefik.${DOMAIN_INTERNAL}\n</code></pre> <p>Info</p> <p>Currently Authelia does not support multi-domain usage. Since the feature is on the roadmap, it will probably be implemented sometime in the future and both instances can be merged back into one. This feature ships with <code>v4.38.0</code>! \ud83c\udf89</p>"},{"location":"cluster/services/authelia/#created-resources","title":"Created Resources","text":"Kind Name <code>Namespace</code> <code>authelia</code> <code>HelmRelease</code> <code>authelia</code>, <code>authelia-external</code> <code>Certificate</code> <code>auth.${DOMAIN_INTERNAL}</code>, <code>auth.${DOMAIN_EXTERNAL}</code> <code>IngressRoute</code> <code>dashboard</code>, <code>dashboard-external</code> <code>ConfigMap</code> <code>authelia-users</code> <code>Middleware</code> <code>headers</code>"},{"location":"cluster/services/authelia/#links","title":"Links","text":"<ul> <li>Documentation</li> <li>Helm Chart</li> <li>GitHub Repository</li> </ul>"},{"location":"cluster/services/blocky/","title":"<code>blocky</code>","text":""},{"location":"cluster/services/blocky/#introduction","title":"Introduction","text":"<p>Blocky is configured to serve as network wide DNS server and ad blocker. To achieve some kind of high availability, multiple instances are deployed as pods using the replicas value of the Helm chart. As upstream DNS server <code>cloudflared</code> is configured.</p> <p>For clients to resolve domains using Blocky. Their DNS server setting needs to be set to the IP of one of the cluster nodes. This can be done via DHCP or manually. Alternatively the routers upstream DNS servers can be set to one nodes IP address.</p> Example DNS queries <pre><code># regular dns using dig\n$ dig @&lt;node-ip&gt; pascaliske.dev\n\n# dns-over-https using dog cli (1)\n$ dog @https://blocky.&lt;domain&gt;/dns-query pascaliske.dev\n</code></pre> <ol> <li>Check ouf the <code>dog</code> cli tool here.</li> </ol>"},{"location":"cluster/services/blocky/#created-resources","title":"Created Resources","text":"Kind Name <code>Namespace</code> <code>blocky</code> <code>HelmRelease</code> <code>blocky</code> <code>Certificate</code> <code>blocky.${DOMAIN_INTERNAL}</code> <code>IngressRoute</code> <code>api</code> <code>ConfigMap</code> <code>blocky-config</code>, <code>grafana-dashboard-dns</code>"},{"location":"cluster/services/blocky/#cli","title":"CLI","text":"<p>Blocky provides a CLI tool to interact with the instance. The following command can be used to access it inside the cluster:</p> <pre><code>$ kubectl exec -it --namespace blocky deploy/blocky -- ./blocky &lt;command&gt;\n</code></pre> <p>For more information on the <code>blocky</code> command itself visit their docs.</p>"},{"location":"cluster/services/blocky/#links","title":"Links","text":"<ul> <li>Documentation</li> <li>Helm Chart</li> <li>GitHub Repository</li> </ul>"},{"location":"cluster/services/clickhouse/","title":"<code>clickhouse</code>","text":""},{"location":"cluster/services/clickhouse/#introduction","title":"Introduction","text":"<p>The Open Source OLAP database management system.</p> <p>ClickHouse is a column-oriented database that enables its users to generate powerful analytics, using SQL queries, in real-time. It scales extremely well and is easily adaptable to any use case.</p>"},{"location":"cluster/services/clickhouse/#created-resources","title":"Created Resources","text":"Kind Name <code>Namespace</code> <code>clickhouse</code> <code>HelmRelease</code> <code>clickhouse</code>"},{"location":"cluster/services/clickhouse/#links","title":"Links","text":"<ul> <li>Documentation</li> <li>Helm Chart</li> <li>GitHub Repository</li> </ul>"},{"location":"cluster/services/cloudflared/","title":"<code>cloudflared</code>","text":""},{"location":"cluster/services/cloudflared/#introduction","title":"Introduction","text":"<p><code>cloudflared</code> is used as upstream DNS server of Blocky. It provides a DNS-over-HTTPS proxy for Cloudflare. To achieve some kind of high availability, multiple instances are deployed as pods using the replicas value of the Helm chart. Those pods are force to be scheduled onto different nodes using inter-pod anti-affinity.</p> <p>As final upstream servers of <code>cloudflared</code> the following addresses of <code>1.1.1.1</code> are configured:</p> <ul> <li><code>https://1.1.1.1/dns-query</code></li> <li><code>https://1.0.0.1/dns-query</code></li> </ul>"},{"location":"cluster/services/cloudflared/#created-resources","title":"Created Resources","text":"Kind Name <code>Namespace</code> <code>cloudflared</code> <code>HelmRelease</code> <code>cloudflared</code>"},{"location":"cluster/services/cloudflared/#links","title":"Links","text":"<ul> <li>Documentation</li> <li>Helm Chart</li> <li>GitHub Repository</li> </ul>"},{"location":"cluster/services/descheduler/","title":"<code>descheduler</code>","text":""},{"location":"cluster/services/descheduler/#introduction","title":"Introduction","text":"<p>The descheduler for Kubernetes is used to re-balance clusters by evicting pods that can potentially be scheduled on better nodes. It can be configured through a policy configuration.</p>"},{"location":"cluster/services/descheduler/#created-resources","title":"Created Resources","text":"Kind Name <code>Namespace</code> <code>descheduler</code> <code>HelmRelease</code> <code>descheduler</code>"},{"location":"cluster/services/descheduler/#links","title":"Links","text":"<ul> <li>Documentation</li> <li>Helm Chart</li> <li>GitHub Repository</li> </ul>"},{"location":"cluster/services/dyndns/","title":"<code>dyndns</code>","text":""},{"location":"cluster/services/dyndns/#introduction","title":"Introduction","text":"<p>Sometimes I need to securely access my home infrastructure while I'm away. For this, I use a VPN connection with a memorable address, which is ensured by using DynDNS. This service in-turn runs a very small container as a CronJob which updates the according DNS record with my current public IP allocated to me by my provider.</p>"},{"location":"cluster/services/dyndns/#created-resources","title":"Created Resources","text":"Kind Name <code>Namespace</code> <code>dyndns</code> <code>HelmRelease</code> <code>dyndns</code>"},{"location":"cluster/services/dyndns/#links","title":"Links","text":"<ul> <li>Helm Chart</li> <li>GitHub Repository</li> </ul>"},{"location":"cluster/services/gitlab/","title":"<code>gitlab</code>","text":""},{"location":"cluster/services/gitlab/#introduction","title":"Introduction","text":"<p>GitLab is a all-in-one platform to plan, build, secure, and deploy software in a DevOps based manner. It includes a full featured CI/CD toolchain.</p> <p>I don't want to upload the code of some of my projects to a platform like GitHub for privacy reasons and therefore I use a self-hosted instance of GitLab for these projects. Besides, the setup and maintenance of a GitLab instance was a good exercise for me.</p>"},{"location":"cluster/services/gitlab/#created-resources","title":"Created Resources","text":"Kind Name <code>Namespace</code> <code>gitlab</code> <code>HelmRelease</code> <code>gitlab</code> <code>Certificate</code> <code>git.${DOMAINS_EXTERNAL}</code> <code>IngressRoute</code> <code>dashboard</code>"},{"location":"cluster/services/gitlab/#cli","title":"CLI","text":"<p>GitLab provides a CLI tool to interact with the instance. The following command can be used to access it inside the cluster:</p> <pre><code>kubectl exec -it --namespace gitlab deploy/gitlab -- gitlab-backup &lt;task&gt; # tasks: create | restore\n</code></pre> <p>For more information on the <code>gitlab-backup</code> command itself visit their docs.</p> <p>Tip</p> <p>These commands could be perfectly automated using a Kubernetes <code>CronJob</code>. You can see an example in my Helm chart for GitLab.</p>"},{"location":"cluster/services/gitlab/#links","title":"Links","text":"<ul> <li>Documentation</li> <li>Helm Chart</li> <li>GitLab Repository</li> </ul>"},{"location":"cluster/services/hammond/","title":"<code>hammond</code>","text":""},{"location":"cluster/services/hammond/#introduction","title":"Introduction","text":"<p>A self-hosted vehicle expense tracking system with support for multiple users.</p> <p>Hammond is a self hosted vehicle management system to track fuel and other expenses related to all of your vehicles. It supports multiple users sharing multiple vehicles. It is the logical successor to Clarkson which has not been updated for quite some time now.</p> <p>Some of it's features are:</p> <ul> <li>Add/Manage multiple vehicles</li> <li>Track fuel and other expenses</li> <li>Share vehicles across multiple users</li> <li>Save attachment against vehicles</li> <li>Quick Entries (take a photo of a receipt or pump screen to make entry later)</li> </ul>"},{"location":"cluster/services/hammond/#created-resources","title":"Created Resources","text":"Kind Name <code>Namespace</code> <code>hammond</code> <code>HelmRelease</code> <code>hammond</code>"},{"location":"cluster/services/hammond/#links","title":"Links","text":"<ul> <li>Documentation</li> <li>Helm Chart</li> <li>GitHub Repository</li> </ul>"},{"location":"cluster/services/home-assistant/","title":"<code>home-assistant</code>","text":""},{"location":"cluster/services/home-assistant/#introduction","title":"Introduction","text":"<p>Home Assistant is my home automation platform of choice. It offers many integrations and is easily configurable. It runs completely local, is privacy-focused and has companion mobile apps.</p> <p>Most of my home automation stuff is ZigBee based which requires a small USB ZigBee Gateway stick: ConBee II. This stick is plugged into one of the cluster nodes. To ensure that the Home Assistant pod is scheduled on this node, it is set up with a dedicated label.</p> Example of a <code>Node</code> with a dedicated label <pre><code>apiVersion: v1\nkind: Node\nmetadata:\n  name: dathomir.iske.cloud\n  labels:\n    k8s.pascaliske.dev/hardware: zigbee\n</code></pre> <p>The configuration files of my Home Assistant instance are stored and managed inside another dedicated GitHub repository. Only the secrets are managed by this repository.</p>"},{"location":"cluster/services/home-assistant/#created-resources","title":"Created Resources","text":"Kind Name <code>Namespace</code> <code>paperless</code> <code>HelmRelease</code> <code>paperless</code> <code>Certificate</code> <code>docs.${DOMAINS_EXTERNAL}</code> <code>IngressRoute</code> <code>dashboard</code> <code>ConfigMap</code> <code>grafana-dashboard-batteries</code>, <code>grafana-dashboard-climate</code>, <code>grafana-dashboard-fuel-prices</code> <code>Secret</code> <code>secrets</code>, <code>monitoring-auth</code>"},{"location":"cluster/services/home-assistant/#cli","title":"CLI","text":"<p>Home Assistant provides a CLI tool to interact with the instance. The following command can be used to access them inside the cluster:</p> <pre><code>kubectl exec -it --namespace home-assistant deploy/home-assistant -- hass --script &lt;script&gt;\n</code></pre> Example <pre><code>kubectl exec -it --namespace home-assistant deploy/home-assistant -- hass --script check_config\n</code></pre> <p>For more information on the <code>hass</code> command itself visit their docs.</p>"},{"location":"cluster/services/home-assistant/#links","title":"Links","text":"<ul> <li>Documentation</li> <li>Helm Chart</li> <li>GitHub Repository</li> </ul>"},{"location":"cluster/services/homer/","title":"<code>homer</code>","text":""},{"location":"cluster/services/homer/#introduction","title":"Introduction","text":"<p>Every homelab needs its own dashboard! I chose <code>homer</code> because it has a clean and minimal user interface and is completely stateless, making it a perfect solution for my cluster.</p> <p></p>"},{"location":"cluster/services/homer/#created-resources","title":"Created Resources","text":"Kind Name <code>Namespace</code> <code>homer</code> <code>HelmRelease</code> <code>homer</code> <code>ConfigMap</code> <code>homer-config</code>"},{"location":"cluster/services/homer/#links","title":"Links","text":"<ul> <li>Documentation</li> <li>Helm Chart</li> <li>GitHub Repository</li> </ul>"},{"location":"cluster/services/linkding/","title":"<code>linkding</code>","text":""},{"location":"cluster/services/linkding/#introduction","title":"Introduction","text":"<p><code>linkding</code> is a great and minimal bookmark service which you can easily self-host. It supports tagging, sharing and automatic internet archive snapshots. It does not require an external database but I set it up with a PostgreSQL database from the <code>cloudnative-pg</code> controller.</p> <p></p>"},{"location":"cluster/services/linkding/#created-resources","title":"Created Resources","text":"Kind Name <code>Namespace</code> <code>linkding</code> <code>HelmRelease</code> <code>linkding</code>"},{"location":"cluster/services/linkding/#links","title":"Links","text":"<ul> <li>Documentation</li> <li>Helm Chart</li> <li>GitHub Repository</li> </ul>"},{"location":"cluster/services/monitoring/","title":"<code>monitoring</code>","text":"<p>This <code>Namespace</code> groups together all monitoring related resources. <code>kube-prometheus-stack</code> and the Prometheus Operator represent the base of the entire monitoring stack and scrape all targets regularly. Grafana is used to visualize those scraped data in multiple application-specific dashboards.</p> <p></p> <p>The following resources belong to this <code>Namespace</code> and are deployed in alphabetical order:</p> <ul> <li><code>fritzbox-exporter</code></li> <li><code>kube-prometheus-stack</code></li> <li><code>loki</code></li> <li><code>magicmirror</code></li> <li><code>minio</code></li> <li><code>redis-exporter</code></li> <li><code>speedtest-exporter</code></li> <li><code>unifi-poller</code></li> <li><code>vector</code></li> </ul>"},{"location":"cluster/services/monitoring/cloudflare-exporter/","title":"<code>cloudflare-exporter</code>","text":""},{"location":"cluster/services/monitoring/cloudflare-exporter/#introduction","title":"Introduction","text":"<p>This exporter extracts analytics data from Cloudflare and provides that as Prometheus readable metrics.</p>"},{"location":"cluster/services/monitoring/cloudflare-exporter/#created-resources","title":"Created Resources","text":"Kind Name <code>HelmRelease</code> <code>fritzbox-exporter</code>"},{"location":"cluster/services/monitoring/cloudflare-exporter/#links","title":"Links","text":"<ul> <li>GitLab Repository</li> <li>Helm Chart</li> </ul>"},{"location":"cluster/services/monitoring/fritzbox-exporter/","title":"<code>fritzbox-exporter</code>","text":""},{"location":"cluster/services/monitoring/fritzbox-exporter/#introduction","title":"Introduction","text":"<p>This exporter extracts analytics data from your FRITZ!Box and provides that as Prometheus readable metrics.</p>"},{"location":"cluster/services/monitoring/fritzbox-exporter/#created-resources","title":"Created Resources","text":"Kind Name <code>HelmRelease</code> <code>fritzbox-exporter</code>"},{"location":"cluster/services/monitoring/fritzbox-exporter/#links","title":"Links","text":"<ul> <li>GitLab Repository</li> <li>Helm Chart</li> </ul>"},{"location":"cluster/services/monitoring/kube-prometheus-stack/","title":"<code>kube-prometheus-stack</code>","text":""},{"location":"cluster/services/monitoring/kube-prometheus-stack/#introduction","title":"Introduction","text":"<p>The <code>kube-prometheus-stack</code> is an easy to manage but powerful way to operate a complete monitoring stack consisting of Prometheus, Alertmanager and Grafana. It makes use of the community managed Prometheus Operator which makes Kubernetes native Prometheus deployments a breeze.</p> <p>The Prometheus Operator looks for <code>ServiceMonitor</code> and <code>PodMonitor</code> objects in all namespaces and adds them to it's list of scrape targets. Most Helm charts already provide those monitor objects behind a values flag. If there was no corresponding flag available a manual declaration was added to this repository as follows:</p> Example of kind <code>ServiceMonitor</code>Example of kind <code>PodMonitor</code> <pre><code>apiVersion: monitoring.coreos.com/v1\nkind: ServiceMonitor\nmetadata:\n  name: cloudflared\n  namespace: cloudflared\nspec:\n  endpoints:\n    - port: http\n      path: /metrics\n      interval: 30s\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: cloudflared # references the target object (1)\n</code></pre> <ol> <li>Selector label which references the target <code>Service</code> object to monitor.</li> </ol> <pre><code>apiVersion: monitoring.coreos.com/v1\nkind: PodMonitor\nmetadata:\n  name: cloudflared\n  namespace: cloudflared\nspec:\n  podMetricsEndpoints:\n    - port: http\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: cloudflared # references the target object (1)\n</code></pre> <ol> <li>Selector label which references the target <code>Pod</code> object to monitor.</li> </ol>"},{"location":"cluster/services/monitoring/kube-prometheus-stack/#created-resources","title":"Created Resources","text":"Kind Name <code>HelmRelease</code> <code>kube-prometheus-stack</code> <code>Certificate</code> <code>prometheus.${DOMAIN_INTERNAL}</code>, <code>alerts.${DOMAIN_INTERNAL}</code>, <code>grafana.${DOMAIN_INTERNAL}</code> <code>IngressRoute</code> <code>dashboard-prometheus</code>, <code>dashboard-alertmanager</code>, <code>dashboard-grafana</code> <code>ConfigMap</code> <code>grafana-dashboard-nodes</code>, <code>grafana-dashboard-containers</code>"},{"location":"cluster/services/monitoring/kube-prometheus-stack/#links","title":"Links","text":"<ul> <li>Documentation</li> <li>GitHub Repository</li> <li>Helm Chart</li> </ul>"},{"location":"cluster/services/monitoring/loki/","title":"<code>loki</code>","text":""},{"location":"cluster/services/monitoring/loki/#introduction","title":"Introduction","text":""},{"location":"cluster/services/monitoring/loki/#created-resources","title":"Created Resources","text":"Kind Name <code>HelmRelease</code> <code>loki</code> <code>Certificate</code> <code>loki.${DOMAIN_INTERNAL}</code> <code>IngressRoute</code> <code>loki</code>"},{"location":"cluster/services/monitoring/loki/#links","title":"Links","text":"<ul> <li>Documentation</li> <li>GitHub Repository</li> <li>Helm Chart</li> </ul>"},{"location":"cluster/services/monitoring/magicmirror/","title":"<code>magicmirror</code>","text":""},{"location":"cluster/services/monitoring/magicmirror/#introduction","title":"Introduction","text":"<p>Another side project of mine is the construction of one of the well known Magic Mirrors. I've completely written the software for my Magic Mirror by myself as I wanted to explore the Go language.</p> <p>The software runs on its own Raspberry Pi and this repository only contains the monitoring dashboard for the Magic Mirror. All other code is in its own repository on GitHub: <code>pascaliske/magicmirror</code>.</p>"},{"location":"cluster/services/monitoring/magicmirror/#created-resources","title":"Created Resources","text":"Kind Name <code>ConfigMap</code> <code>grafana-dashboard-magicmirror</code>"},{"location":"cluster/services/monitoring/magicmirror/#links","title":"Links","text":"<ul> <li>GitHub Repository</li> </ul>"},{"location":"cluster/services/monitoring/minio/","title":"<code>minio</code>","text":""},{"location":"cluster/services/monitoring/minio/#introduction","title":"Introduction","text":"<p>On my Synology NAS there is a MinIO instance running. It is managed by the Provision playbook inside this repo and serves as a <code>BackupStorageLocation</code> for Velero. This <code>Namespace</code> only contains the monitoring dashboard for it.</p>"},{"location":"cluster/services/monitoring/minio/#created-resources","title":"Created Resources","text":"Kind Name <code>ConfigMap</code> <code>grafana-dashboard-minio</code>"},{"location":"cluster/services/monitoring/minio/#links","title":"Links","text":"<ul> <li>Documentation</li> <li>GitHub Repository</li> </ul>"},{"location":"cluster/services/monitoring/plausible-exporter/","title":"<code>plausible-exporter</code>","text":""},{"location":"cluster/services/monitoring/plausible-exporter/#introduction","title":"Introduction","text":"<p>This exporter extracts data from your Plausible Analytics instance and provides that as Prometheus readable metrics.</p>"},{"location":"cluster/services/monitoring/plausible-exporter/#created-resources","title":"Created Resources","text":"Kind Name <code>HelmRelease</code> <code>plausible-exporter</code>"},{"location":"cluster/services/monitoring/plausible-exporter/#links","title":"Links","text":"<ul> <li>GitLab Repository</li> <li>Helm Chart</li> </ul>"},{"location":"cluster/services/monitoring/redis-exporter/","title":"<code>redis-exporter</code>","text":""},{"location":"cluster/services/monitoring/redis-exporter/#introduction","title":"Introduction","text":"<p>This exporter extracts analytics data from Redis and provides that as Prometheus readable metrics.</p>"},{"location":"cluster/services/monitoring/redis-exporter/#created-resources","title":"Created Resources","text":"Kind Name <code>HelmRelease</code> <code>prometheus-redis-exporter</code>"},{"location":"cluster/services/monitoring/redis-exporter/#links","title":"Links","text":"<ul> <li>GitHub Repository</li> <li>Helm Chart</li> </ul>"},{"location":"cluster/services/monitoring/speedtest-exporter/","title":"<code>speedtest-exporter</code>","text":""},{"location":"cluster/services/monitoring/speedtest-exporter/#introduction","title":"Introduction","text":"<p>This exporter runs an internet speed test every time Prometheus scrapes it. The speed test is performed by the well known speedtest.net by Ookla.</p>"},{"location":"cluster/services/monitoring/speedtest-exporter/#created-resources","title":"Created Resources","text":"Kind Name <code>HelmRelease</code> <code>speedtest-exporter</code> <code>ConfigMap</code> <code>grafana-dashboard-internet</code>"},{"location":"cluster/services/monitoring/speedtest-exporter/#links","title":"Links","text":"<ul> <li>Documentation</li> <li>GitHub Repository</li> <li>Helm Chart</li> </ul>"},{"location":"cluster/services/monitoring/unifi-poller/","title":"<code>unifi-poller</code>","text":""},{"location":"cluster/services/monitoring/unifi-poller/#introduction","title":"Introduction","text":""},{"location":"cluster/services/monitoring/unifi-poller/#created-resources","title":"Created Resources","text":"Kind Name <code>HelmRelease</code> <code>unifi-poller</code> <code>ConfigMap</code> <code>grafana-dashboard-unifi</code>"},{"location":"cluster/services/monitoring/unifi-poller/#links","title":"Links","text":"<ul> <li>GitHub Repository</li> <li>Helm Chart</li> </ul>"},{"location":"cluster/services/monitoring/vector/","title":"<code>vector</code>","text":""},{"location":"cluster/services/monitoring/vector/#introduction","title":"Introduction","text":""},{"location":"cluster/services/monitoring/vector/#created-resources","title":"Created Resources","text":"Kind Name <code>HelmRelease</code> <code>vector</code> <code>ConfigMap</code> <code>grafana-dashboard-logs</code>"},{"location":"cluster/services/monitoring/vector/#links","title":"Links","text":"<ul> <li>GitHub Repository</li> <li>Helm Chart</li> </ul>"},{"location":"cluster/services/pairdrop/","title":"<code>pairdrop</code>","text":""},{"location":"cluster/services/pairdrop/#introduction","title":"Introduction","text":"<p>A progressive web app for local file sharing inside the browser. Files are sent only between peers and are never sent to any server.</p> <p>I'm a huge fan of Apples AirDrop for simply sharing files between devices. But due to some non Apple devices I needed an alternative and found <code>pairdrop</code>. It provides a easy to use a d secure way to transfer files between devices with potentially different operating systems.</p>"},{"location":"cluster/services/pairdrop/#created-resources","title":"Created Resources","text":"Kind Name <code>Namespace</code> <code>pairdrop</code> <code>HelmRelease</code> <code>pairdrop</code>"},{"location":"cluster/services/pairdrop/#links","title":"Links","text":"<ul> <li>Documentation</li> <li>Helm Chart</li> <li>GitHub Repository</li> </ul>"},{"location":"cluster/services/paperless/","title":"<code>paperless</code>","text":""},{"location":"cluster/services/paperless/#introduction","title":"Introduction","text":"<p>A community-supported supercharged version of paperless: scan, index and archive all your physical documents.</p> <p>Paperless-ngx is a document management system that transforms your physical documents into a searchable online archive so you can keep, well, less paper \u2013 although I still keep almost all documents in paper form.</p> <p>Some of it's features are:</p> <ul> <li>Performs OCR on your documents, adds selectable text to image only documents and adds tags, correspondents and document types to your documents.</li> <li>Full text search helps you find what you need.</li> <li>Email processing: Paperless adds documents from your email accounts.</li> <li>Machine learning powered document matching.</li> <li>More features can be found on their GitHub repository</li> </ul> <p></p>"},{"location":"cluster/services/paperless/#created-resources","title":"Created Resources","text":"Kind Name <code>Namespace</code> <code>paperless</code> <code>HelmRelease</code> <code>paperless</code>"},{"location":"cluster/services/paperless/#cli","title":"CLI","text":"<p>Paperless provides several CLI tools to interact with the instance. These are the most important ones:</p> Exporting documentsImporting documentsRe-tag documentsFetching e-mails <pre><code>kubectl exec -it --namespace paperless deploy/paperless -- document_exporter\n</code></pre> <p>For more information on the <code>document_exporter</code> command visit their docs.</p> <pre><code>kubectl exec -it --namespace paperless deploy/paperless -- document_importer\n</code></pre> <p>For more information on the <code>document_importer</code> command visit their docs.</p> <pre><code>kubectl exec -it --namespace paperless deploy/paperless -- document_retagger\n</code></pre> <p>For more information on the <code>document_retagger</code> command visit their docs.</p> <pre><code>kubectl exec -it --namespace paperless deploy/paperless -- mail_fetcher\n</code></pre> <p>For more information on the <code>mail_fetcher</code> command visit their docs.</p>"},{"location":"cluster/services/paperless/#links","title":"Links","text":"<ul> <li>Documentation</li> <li>Helm Chart</li> <li>GitHub Repository</li> </ul>"},{"location":"cluster/services/plausible/","title":"<code>plausible</code>","text":""},{"location":"cluster/services/plausible/#introduction","title":"Introduction","text":"<p>Easy to use and privacy-friendly Google Analytics alternative.</p> <p>Plausible is an intuitive, lightweight and open source web analytics software. It does not require cookies and therefore is fully compliant with GDPR, CCPA and PECR. See their data policy for more information.</p> <p></p>"},{"location":"cluster/services/plausible/#created-resources","title":"Created Resources","text":"Kind Name <code>Namespace</code> <code>plausible</code> <code>HelmRelease</code> <code>plausible</code> <code>Secret</code> <code>plausible-secrets</code>"},{"location":"cluster/services/plausible/#links","title":"Links","text":"<ul> <li>Documentation</li> <li>Helm Chart</li> <li>GitHub Repository</li> </ul>"},{"location":"cluster/services/redis/","title":"<code>redis</code>","text":""},{"location":"cluster/services/redis/#introduction","title":"Introduction","text":"<p>The open source, in-memory data store used by millions of developers as a database, cache, streaming engine, and message broker.</p> <p>Redis is used by multiple services (e.g. Authelia) to cache sessions, store short-lived data or as storage between multiple replicas of an application.</p>"},{"location":"cluster/services/redis/#created-resources","title":"Created Resources","text":"Kind Name <code>Namespace</code> <code>redis</code> <code>HelmRelease</code> <code>redis</code>"},{"location":"cluster/services/redis/#cli","title":"CLI","text":"<p>Redis provides a CLI tool to interact with the instance. The following commands can be used to access it inside the cluster:</p> InteractiveOne-off <pre><code>kubectl exec -it --namespace redis deploy/redis -- redis-cli\n</code></pre> <pre><code>kubectl exec -it --namespace redis deploy/redis -- redis-cli &lt;command&gt;\n</code></pre> <p>For more information on the <code>redis-cli</code> command itself visit their docs.</p>"},{"location":"cluster/services/redis/#links","title":"Links","text":"<ul> <li>Documentation</li> <li>Helm Chart</li> <li>GitHub Repository</li> </ul>"},{"location":"cluster/services/unbound/","title":"<code>unbound</code>","text":""},{"location":"cluster/services/unbound/#introduction","title":"Introduction","text":"<p><code>unbound</code> is used as upstream DNS server of Blocky. It provides a validating and caching DNS proxy. To achieve some kind of high availability, multiple instances are deployed as pods using the replicas value of the Helm chart. Those pods are force to be scheduled onto different nodes using inter-pod anti-affinity.</p>"},{"location":"cluster/services/unbound/#created-resources","title":"Created Resources","text":"Kind Name <code>Namespace</code> <code>unbound</code> <code>HelmRelease</code> <code>unbound</code>"},{"location":"cluster/services/unbound/#links","title":"Links","text":"<ul> <li>Documentation</li> <li>Helm Chart</li> <li>GitHub Repository</li> </ul>"},{"location":"cluster/services/unifi/","title":"<code>unifi</code>","text":""},{"location":"cluster/services/unifi/#introduction","title":"Introduction","text":"<p>The UniFi ecosystem is a great and valuable choice if you want to level up your home network from consumer grade hardware to a more professional one.</p> <p>Both their switches and access points are awesome products and can be managed from a single pane of glass: the UniFi controller software. It can easily be self-hosted thanks to @jacobalberty.</p>"},{"location":"cluster/services/unifi/#created-resources","title":"Created Resources","text":"Kind Name <code>Namespace</code> <code>homer</code> <code>HelmRelease</code> <code>homer</code> <code>Certificate</code> <code>${DOMAIN_INTERNAL}</code> <code>IngressRoute</code> <code>dashboard</code>"},{"location":"cluster/services/unifi/#links","title":"Links","text":"<ul> <li>Helm Chart</li> <li>GitHub Repository</li> </ul>"},{"location":"cluster/services/uptime-kuma/","title":"<code>uptime-kuma</code>","text":""},{"location":"cluster/services/uptime-kuma/#introduction","title":"Introduction","text":"<p>In addition to a dashboard, each homelab needs a status page. Uptime Kuma is a great candidate for this. It provides a minimal but beautiful interface, can be hichly customized and every major notification platform is supported. One minor issue is the lack of support for a programmatic way to configure monitors, but hopefully this will be added some time in the future...</p>"},{"location":"cluster/services/uptime-kuma/#created-resources","title":"Created Resources","text":"Kind Name <code>Namespace</code> <code>uptime-kuma</code> <code>HelmRelease</code> <code>uptime-kuma</code>"},{"location":"cluster/services/uptime-kuma/#links","title":"Links","text":"<ul> <li>Documentation</li> <li>Helm Chart</li> <li>GitHub Repository</li> </ul>"},{"location":"cluster/services/vaultwarden/","title":"<code>vaultwarden</code>","text":""},{"location":"cluster/services/vaultwarden/#introduction","title":"Introduction","text":"<p>Unofficial Bitwarden compatible server written in Rust, formerly known as bitwarden_rs.</p> <p>Coming from 1Password (which is a great software!) I found an even better password manager: Vaultwarden/Bitwarden. It combines awesome features and privacy awareness into a beautiful UI which dan be used on almost any device.</p>"},{"location":"cluster/services/vaultwarden/#created-resources","title":"Created Resources","text":"Kind Name <code>Namespace</code> <code>vaultwarden</code> <code>HelmRelease</code> <code>vaultwarden</code> <code>Certificate</code> <code>vault.${DOMAIN_INTERNAL}</code> <code>IngressRoute</code> <code>dashboard</code>"},{"location":"cluster/services/vaultwarden/#links","title":"Links","text":"<ul> <li>Documentation</li> <li>Helm Chart</li> <li>GitHub Repository</li> </ul>"},{"location":"cluster/services/vikunja/","title":"<code>vikunja</code>","text":""},{"location":"cluster/services/vikunja/#introduction","title":"Introduction","text":"<p>The open-source, self-hostable to-do app.</p> <p>Notes and to-do lists play a big role in my daily job. They help me to structure and speed up my work. Recently I made the switch from simple text files to Vikunja. With it's multiple types of views and many options to enrich to-dos with metadata I hope to be even more productive than before.</p>"},{"location":"cluster/services/vikunja/#created-resources","title":"Created Resources","text":"Kind Name <code>Namespace</code> <code>vikunja</code> <code>HelmRelease</code> <code>vikunja</code> <code>Cluster</code> <code>postgresql</code> <code>Secret</code> <code>postgresql-superuser</code>, <code>postgresql-user</code>"},{"location":"cluster/services/vikunja/#links","title":"Links","text":"<ul> <li>Documentation</li> <li>Helm Chart</li> <li>Git Repositories</li> </ul>"},{"location":"provisioning/","title":"Provisioning","text":"<p>The hardware in the cluster is fully provisioned and managed using Ansible. Specifically the provisioning and updating procedures of all cluster nodes are automated using two separate playbooks.</p>"},{"location":"provisioning/#inventory","title":"Inventory","text":"<p>Inside this file all hosts for the cluster and their connection details are defined.</p> <pre><code># cluster\n[masters]\ncoruscant.iske.cloud ansible_user=pi\nkashyyyk.iske.cloud ansible_user=pi\nalderaan.iske.cloud ansible_user=pi\n\n[workers]\ndathomir.iske.cloud ansible_user=pi\nmustafar.iske.cloud ansible_user=pi\njakku.iske.cloud ansible_user=pi\n\n[cluster:children]\nmasters\nworkers\n\n# backup\n[backup]\nryloth.iske.cloud ansible_user=pascaliske\n</code></pre>"},{"location":"provisioning/#playbooks","title":"Playbooks","text":""},{"location":"provisioning/#provisionyml","title":"<code>provision.yml</code>","text":"TL;DR \u2014 <code>ansible/playbooks/provision.yml</code> <pre><code># masters\n- name: Provision Masters\n  hosts: masters\n  roles:\n    - role: common\n      tags:\n        - masters\n        - common\n    - role: log2ram\n      tags:\n        - masters\n        - log2ram\n    - role: journal\n      tags:\n        - masters\n        - journal\n    - role: logrotate\n      tags:\n        - masters\n        - logrotate\n    - role: tailscale\n      tags:\n        - masters\n        - tailscale\n    - role: keepalived\n      tags:\n        - masters\n        - keepalived\n    - role: k3s\n      tags:\n        - masters\n        - k3s\n\n# workers\n- name: Provision Workers\n  hosts: workers\n  roles:\n    - role: common\n      tags:\n        - workers\n        - common\n    - role: log2ram\n      tags:\n        - workers\n        - log2ram\n    - role: journal\n      tags:\n        - workers\n        - journal\n    - role: logrotate\n      tags:\n        - workers\n        - logrotate\n    - role: tailscale\n      tags:\n        - workers\n        - tailscale\n    - role: k3s\n      tags:\n        - workers\n        - k3s\n\n# backup\n- name: Provision Backup\n  hosts: backup\n  become: true\n  roles:\n    - role: minio\n      tags:\n        - minio\n</code></pre> <p>For a initial and complete provisioning of all nodes the following command can be used:</p> <pre><code>$ task cluster:provision\n</code></pre> <p>To only run specific parts of the playbook the <code>--tags</code> flag can be appended to the command:</p> <pre><code>$ task cluster:provision -- --tags &lt;tag1&gt;[,&lt;tag2&gt;]\n</code></pre> <p>The following tags are available for usage with <code>--tags</code>:</p> <ul> <li><code>masters</code></li> <li><code>workers</code></li> <li><code>common</code></li> <li><code>journal</code></li> <li><code>log2ram</code></li> <li><code>logrotate</code></li> <li><code>tailscale</code></li> <li><code>k3s</code></li> <li><code>minio</code></li> </ul>"},{"location":"provisioning/#updateyml","title":"<code>update.yml</code>","text":"TL;DR \u2014 <code>ansible/playbooks/update.yml</code> <pre><code># masters\n- name: Update Masters\n  hosts: masters\n  become: true\n  tasks:\n    - name: Update apt packages\n      apt:\n        upgrade: safe\n        update_cache: true\n        autoremove: true\n      tags:\n        - masters\n\n# workers\n- name: Update Workers\n  hosts: workers\n  become: true\n  tasks:\n    - name: Update apt packages\n      apt:\n        upgrade: safe\n        update_cache: true\n        autoremove: true\n      tags:\n        - workers\n</code></pre> <p>The update playbook allows me to simply update / patch all nodes:</p> <pre><code>$ task cluster:update\n</code></pre> <p>To only run specific parts of the playbook the <code>--tags</code> flag can be appended to the command:</p> <pre><code>$ task cluster:update -- --tags &lt;tag1&gt;[,&lt;tag2&gt;]\n</code></pre> <p>The following tags are available for usage with <code>--tags</code>:</p> <ul> <li><code>masters</code></li> <li><code>workers</code></li> </ul>"},{"location":"provisioning/#cleanupyml","title":"<code>cleanup.yml</code>","text":"TL;DR \u2014 <code>ansible/playbooks/cleanup.yml</code> <pre><code># masters\n- name: Clean-up Masters\n  hosts: masters\n  roles:\n    - role: logs\n      tags:\n        - masters\n        - logs\n\n# workers\n- name: Clean-up Workers\n  hosts: workers\n  roles:\n    - role: logs\n      tags:\n        - workers\n        - logs\n</code></pre> <p>Sometimes, <code>logrotate</code> and <code>log2ram</code> can't keep up with the log files. For this rare cases I have an cleanup playbook which allows me to cleanup the <code>/var/log</code> folders of all cluster nodes to prevent an overflow of the available disk space:</p> <pre><code>$ task cluster:cleanup\n</code></pre> <p>To only run specific parts of the playbook the <code>--tags</code> flag can be appended to the command:</p> <pre><code>$ task cluster:cleanup -- --tags &lt;tag1&gt;[,&lt;tag2&gt;]\n</code></pre> <p>The following tags are available for usage with <code>--tags</code>:</p> <ul> <li><code>masters</code></li> <li><code>workers</code></li> <li><code>logs</code></li> </ul>"},{"location":"provisioning/#limit","title":"Limit","text":"<p>All playbooks can be executed on a limited set of hosts using the <code>--limit</code> flag:</p> <pre><code>$ task cluster:&lt;provision|update|cleanup&gt; -- --limit &lt;host1&gt;[,&lt;host2&gt;]\n</code></pre> <p>Any hosts from the inventory can be used with this flag.</p>"},{"location":"provisioning/#vault","title":"Vault","text":"<p>Some values needed for the above playbooks are stored as an encrypted secrets file using Ansible Vault.</p> <p>To encrypt or decrypt I use the following commands:</p> EncryptionDecryption <pre><code>$ task vault:encrypt\n</code></pre> <pre><code>$ task vault:decrypt\n</code></pre>"},{"location":"secrets/","title":"Secrets","text":"<p>Almost all services require some kind of secret value at runtime. In order to store those secrets safely inside the Git repository I use Mozilla SOPS which is natively supported by Flux \u2013 more precisely, I use <code>age</code> as an algorithm for encryption.</p> <p>Info</p> <p>Please keep in mind that this page is only meant as an example implementation of my workflow. You're free to adapt it, but you will need to adjust it by yourself.</p>"},{"location":"secrets/#prerequisites","title":"Prerequisites","text":"<p>It is required to have SOPS and <code>age</code> installed on your machine. You can leverage <code>brew</code> for that:</p> <pre><code>$ brew install sops age\n</code></pre> <p>You also need to generate a key for <code>age</code> and enable the Flux controllers to decrypt your secrets.</p> <p>Store <code>age</code> key in environment variable</p> <p>This repository provides some task definitions to easily maintain SOPS encrypted files. To use them you just need to ensure your <code>age</code> public key is available in an environment variable called <code>AGE_PUBLIC_KEY</code>.</p>"},{"location":"secrets/#workflow","title":"Workflow","text":"<p>Let's assume you want to deploy an application which needs the following secret to connect to a database:</p> db-user.sops.yaml<pre><code>apiVersion: v1\nkind: Secret\nmetadata:\n  name: postgresql-user\n  namespace: my-app\ntype: kubernetes.io/basic-auth\nstringData:\n  username: my-app\n  password: $uper$ecret123\n</code></pre> <p>To store the secret safely in the Git repository, you would do the following:</p> <ol> <li>You create the secret manifest locally and name it e.g. <code>db-user.sops.yaml</code></li> <li>Encrypt the file using SOPS &amp; <code>age</code></li> <li>Specify the secret inside the <code>Deployment</code> as you normally would</li> <li>Commit the files to the Git repository</li> </ol> <p>As soon as the commit is pushed to the Git repository, the following happens inside the cluster:</p> <ol> <li><code>source-controller</code> pulls the changes from Git</li> <li><code>kustomize-controller</code> loads the <code>age</code> keys from the <code>sops-age</code> secret</li> <li><code>kustomize-controller</code> decrypts your secret and applies it on the cluster</li> </ol>"},{"location":"secrets/#commands","title":"Commands","text":""},{"location":"secrets/#encryption","title":"Encryption","text":"<p>The following commands allow you to encrypt any YAML or JSON file and store it inside the Git repository:</p> Kubernetes <code>Secret</code>Generic YAML <pre><code>$ task sops:encrypt:secret -- path/to/my/secret.sops.yaml\n</code></pre> <pre><code>$ task sops:encrypt:generic -- path/to/my/secret.sops.yaml\n</code></pre>"},{"location":"secrets/#decryption","title":"Decryption","text":"<p>If you need to modify the secret value later on, you can decrypt it locally \u2013 as long as the private age key exists on your machine:</p> Kubernetes <code>Secret</code>Generic YAML <pre><code>$ task sops:decrypt:secret -- path/to/my/secret.sops.yaml\n</code></pre> <pre><code>$ task sops:decrypt:generic -- path/to/my/secret.sops.yaml\n</code></pre>"},{"location":"specs/","title":"Specs","text":""},{"location":"specs/#hardware","title":"Hardware","text":"<p>The following hardware components are used:</p> <ul> <li>5x Raspberry Pi 4 Model B (1x 4 GB, 4x 8 GB)</li> <li>5x Raspberry Pi PoE+ Hat</li> <li>5x SanDisk Extreme microSDXC (1x 64 GB, 4x 128 GB)</li> <li>1x UniFi Switch Lite 16 PoE</li> <li>1x GeekPi 6 Layer Cluster Case</li> <li>1x Netcup VPS 1000 G10 (6 vCores, 8 GB RAM)</li> <li>1x Synology DiskStation DS920+ (2x 3 GB &amp; 2x 4 GB)</li> </ul> <p>Note</p> <p>The fans of the official PoE hats appear to be very noisy. Therefore I adjusted their temperature thresholds to 70\u00b0C and 80\u00b0C:</p> /boot/config.txt<pre><code>dtparam=poe_fan_temp0=70000,poe_fan_temp0_hyst=5000\ndtparam=poe_fan_temp1=80000,poe_fan_temp1_hyst=2000\n</code></pre>"},{"location":"specs/#software","title":"Software","text":"<ul> <li>Raspberry Pi OS Lite (64-bit) / Ubuntu Server 22.04 LTS</li> <li>Ansible from RedHat</li> <li>K3s originally from Rancher, now a CNCF project</li> <li>Flux originally from Weaveworks, now a CNCF project</li> <li>SOPS from Mozilla</li> <li>GitHub Actions</li> </ul>"},{"location":"specs/#networking","title":"Networking","text":""},{"location":"specs/#physical","title":"Physical","text":"<p>The Raspberry Pis are all physically connected to a UniFi PoE switch for power and data. The actual communication between all cluster nodes is happening through the Tailscale network.</p>"},{"location":"specs/#virtual","title":"Virtual","text":"<p>Inside the cluster the main parts of the networking setup are CoreDNS and Traefik. The following IPs and CIDRs are configured inside K3s:</p> Name IP / CIDR Kubernetes Pod IPs <code>10.42.0.0/16</code> Kubernetes Service IPs <code>10.43.0.0/16</code> Kubernetes Cluster DNS IP <code>10.43.0.10</code>"},{"location":"specs/#dns","title":"DNS","text":""},{"location":"specs/#internal","title":"Internal","text":"<p>All services with a UI running in my cluster have a dedicated subdomain configured. Those domains are managed by Blocky which is configured as the main DNS server in my home network. It also performs network-wide ad blocking based on some block lists.</p>"},{"location":"specs/#external","title":"External","text":"<p>Services that need to be reachable externally have dedicated public DNS records. These are configured using Terraform.</p> <p>Example of a DNS Terraform resource</p> <pre><code>data \"cloudflare_zone\" \"zone_public\" {\n  name = \"my-domain.com\" # target DNS zone (1)\n}\n\nresource \"cloudflare_record\" \"public\" {\n  zone_id = data.cloudflare_zone.zone_public.id\n  type    = \"A\"\n  name    = \"*\" # record name (2)\n  value   = \"1.2.3.4\" # record value (3)\n}\n</code></pre> <ol> <li>This block references the target DNS zone</li> <li>Record name (a.k.a subdomain) to be configured</li> <li>Record value to be configured</li> </ol>"},{"location":"specs/#dynamic","title":"Dynamic","text":"<p>Since I don't have a static IP address allocated from my ISP, my IP address can change at any point in time. To bypass this circumstance I configured a Kubernetes <code>CronJob</code> object which performs an DynDNS update for my domain at Cloudflare. The configuration can be found in the <code>dyndns</code> section.</p>"},{"location":"specs/#storage","title":"Storage","text":"<p>Currently I'm not using anything special as a storage solution for my cluster. The built-in <code>local-path-provisioner</code> of K3s is fulfilling any <code>PersistentVolumeClaim</code> objects but I'm planning to switch to Longhorn.</p>"},{"location":"specs/#hostnames","title":"Hostnames","text":"<p>Every node has configured a hostname which follows a particular naming scheme:</p> /etc/hostname<pre><code>&lt;planet&gt;.iske.cloud\n</code></pre> <p>As you may have guessed, the planets are taken from the Star Wars universe. Currently the following names are in use:</p> <ul> <li><code>coruscant</code></li> <li><code>kashyyyk</code></li> <li><code>alderaan</code></li> <li><code>dathomir</code></li> <li><code>mustafar</code></li> <li><code>jakku</code></li> <li><code>ryloth</code></li> <li><code>ilum</code></li> </ul>"},{"location":"updates/","title":"Updates","text":""},{"location":"updates/#operating-system","title":"Operating System","text":"<p>The underlying cluster nodes can be fully updated by using the following Ansible playbook:</p> <pre><code># update nodes using ansible (1)\n$ task cluster:update\n</code></pre> <ol> <li>More information on this command can be found in the provisioning section.</li> </ol> <p>For critical and/or security relevant updates the unattended-upgrades tool is configured on all nodes:</p> /etc/apt/apt.conf.d/50unattended-upgrades<pre><code>Unattended-Upgrade::Automatic-Reboot \"false\";\nUnattended-Upgrade::Automatic-Reboot-Time \"03:00\";\n\nUnattended-Upgrade::Allowed-Origins {\n    \"${distro_id} ${distro_codename}-security\";\n    \"${distro_id} ${distro_codename}-updates\";\n};\n\nUnattended-Upgrade::Package-Blacklist {\n};\n</code></pre>"},{"location":"updates/#kubernetes","title":"Kubernetes","text":"<p>Rancher's <code>system-upgrade-controller</code> is leveraged to update the K3s runtime on every node. See the implementation details in the cluster section for more detailed information.</p> <p>Additionally, Renovate Bot is configured to automatically create Pull Requests for new versions of K3s \u2013 you can view an example here.</p> <p>As soon as a pull request with an K3s update is merged, Flux starts reconciling the <code>Plan</code> manifests, the <code>system-upgrade-controller</code> detects the new version inside them and starts updating all nodes one by one, starting with the master nodes.</p> <p></p>"},{"location":"updates/#services","title":"Services","text":"<p>Updates of the running services and containers are also done via Pull Requests by Renovate Bot which fits perfectly into the GitOps based workflow of Flux. It continuously checks the following data sources for new versions and creates Pull Requests to adapt them inside the cluster:</p> <ul> <li>Container images</li> <li>Helm Charts</li> <li>GitHub repositories</li> <li>GitHub releases</li> </ul>"}]}